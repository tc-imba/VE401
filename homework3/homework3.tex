\documentclass[11pt,a4paper]{article}

\usepackage{../ve401}

\author{Group 37}
\semester{Spring}
\year{2019}
\subtitle{Assignment}
\subtitlenumber{3}
\blockinfo{
	\bigskip
	\begin{center}
		\textbf{Group members}
	\end{center}
	\begin{itemize}\itemsep .25cm
		\item \href{mailto:hcm_9809@sjtu.edu.cn}{Chenmin Hou} (517370910248)
		\item \href{mailto:liuyh615@126.com}{Yihao Liu} (515370910207)
		\item \href{mailto:lyz0123@sjtu.edu.cn}{Yuzhou Li} (517021910922)
	\end{itemize}
}

\begin{document}

\maketitle

\subsection{}

\begin{enumerate}[label=\roman*)]
\item
From the definition of $f_{XY}(x,y)$, we know $f_{XY}(x,y)\geqslant0$ for all $(x,y)\in\Omega$, and
$$\sum_{(x,y)\in\Omega}f_{XY}(x,y)=\sum_{x=1}^n\sum_{y=x}^n\frac{2}{n(n+1)}=\sum_{x=1}^nx\frac{2}{n(n+1)}=\frac{n(n+1)}{2}\cdot\frac{2}{n(n+1)}=1.$$
So $f_{XY}$ is in fact a density.
\item
\begin{align*}
f_X(x)&=\sum_yf_{XY}(x,y)=\sum_{y=x}^n\frac{2}{n(n+1)}=\frac{2(n+1-x)}{n(n+1)},\\
f_Y(y)&=\sum_xf_{XY}(x,y)=\sum_{x=1}^y\frac{2}{n(n+1)}=\frac{2y}{n(n+1)}.
\end{align*}
\item
$$f_X(x)f_Y(y)=\frac{4(n+1-x)y}{n^2(n+1)^2},$$
$$f_{XY}(n,1)=\frac{2}{n(n+1)}\neq f_X(n)f_Y(1)=\frac{4}{n^2(n+1)^2}.$$
So $X$ and $Y$ are not independent.
\item
\begin{align*}
P[X\leqslant3{\rm\ and\ }Y\leqslant2]&=\sum_{x=1}^3\sum_{y=x}^2f_{XY}(x,y)=\sum_{x=1}^3\sum_{y=x}^2\frac{2}{n(n+1)}=3\cdot\frac{2}{5\cdot(5+1)}=\frac{1}{5},\\
P[X\leqslant3]&=\sum_{x=1}^3f_X(x)=\sum_{x=1}^3\frac{2(n+1-x)}{n(n+1)}=(5+4+3)\cdot\frac{2}{5\cdot(5+1)}=\frac{4}{5},\\
P[Y\leqslant2]&=\sum_{y=1}^2f_Y(y)=\sum_{y=1}^2\frac{2y}{n(n+1)}=(1+2)\cdot\frac{2}{5\cdot(5+1)}=\frac{1}{5}.
\end{align*}
\end{enumerate}

\subsection{The Sum of Two Continuous Random Variables}

Consider the transformation 
$$\varphi(X,Y)\mapsto (U,V)$$
where
$$\varphi(x,y)=\binom{x+y}{y}.$$
Then
$$\varphi^{-1}(u,v)=\binom{u-v}{v}.$$
We calculate
$$D\varphi^{-1}(u,v)=\begin{pmatrix}
\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\
\frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} \\
\end{pmatrix}=\begin{pmatrix}
1 & -1 \\ 0 & 1 \\
\end{pmatrix},$$
so
$$\left|\frac{\partial(x,y)}{\partial(u,v)}\right||\det D\varphi^{-1}(u,v)|=1.$$
Then
$$f_{UV}(u,v)=f_{XY}(x,y)|\det D\varphi^{-1}(u,v)|=f_{XY}(x,y).$$
The marginal density $f_U$ is given by
$$f_U(u)=\int_{-\infty}^\infty f_{UV}(u,v)dv=\int_{-\infty}^\infty f_{XY}(x,y)dy=\int_{-\infty}^\infty f_{XY}(u-v,v)dv.$$

\subsection{The Sum of Two Exponential Distributions}
Let
$$f_X(x)=\left\{\begin{aligned}
&\beta_1e^{-\beta_1x}=\frac{1}{3}e^{-x/3} &\quad x>0 \\
&0 &\quad x\leqslant 0
\end{aligned}\right.,\quad
f_Y(y)=\left\{\begin{aligned}
&\beta_2e^{-\beta_2y}=e^{-y} &\quad x>0 \\
&0 &\quad x\leqslant 0
\end{aligned}\right.,$$
and
$$f_{XY}(x,y)=f_X(x)f_Y(y)=\left\{\begin{aligned}
&\frac{1}{3}e^{-x/3-y} &\quad x>0 \\
&0 &\quad x\leqslant 0
\end{aligned}\right..$$
According to Exercise 3.2,
$$f_U(u)=\int_{-\infty}^\infty f_{XY}(u-v,v)dv.$$
When $u,v>0$, $u-v>0$, we can get $0<v<u$, then
$$f_U(u)=\int_0^u\frac{1}{3}e^{-(u-v)/3-v}dv=\frac{1}{3}e^{-u/3}\int_0^u e^{-2v/3}dv=\frac{1}{3}e^{-u/3}\cdot-\frac{3}{2}e^{-2v/3}\bigg|_0^u=\frac{1}{2}(e^{-u/3}-e^{-u}).$$
When $u,v\leqslant0$, $f_U(u)=0$, so
$$f_U(u)=\left\{\begin{aligned}
&\frac{1}{2}(e^{-u/3}-e^{-u}) &\quad u>0 \\
&0 &\quad u\leqslant 0
\end{aligned}\right..$$

\subsection{The Linear Combination of Two Normal Distributions}

$$f_{X_1}(x)=\frac{1}{\sqrt{2\pi}\sigma_1}\exp\left[-\frac{1}{2}\left(\frac{x-\mu_1}{\sigma_1}\right)^2\right].$$
Let $Y_1=\lambda_1X_1$, so that $Y_1=\varphi\circ X_1$, where $\varphi:R\mapsto R$, $\varphi(x)=\lambda_1 x$, according to Theorem 1.3.13,
$$f_{Y_1}(y)=f_{X_1}(\varphi^{-1}(y))\cdot\left|\frac{d\varphi^{-1}(y)}{dy}\right|=\frac{1}{\lambda_1}f_{X_1}\left(\frac{y}{\lambda_1}\right)=\frac{1}{\sqrt{2\pi}\sigma_1\lambda_1}\exp\left[-\frac{1}{2}\left(\frac{y-\mu_1\lambda_1}{\sigma_1\lambda_1}\right)^2\right].$$
So $Y_1=\lambda_1X_1$ follows a normal distribution with mean $\mu_1\lambda_1$ and variance $\sigma_1^2\lambda_1^2$.

Similarly, $Y_2=\lambda_2X_2$ follows a normal distribution with mean $\mu_2\lambda_2$ and variance $\sigma_2^2\lambda_2^2$.

The moment generating function of $Y=\lambda_1X_1+\lambda_2X_2=Y_1+Y_2$ is
$$m_Y(t)=E[e^{tY}]=E[e^{t(Y_1+Y_2)}].$$

Since $X_1$ and $X_2$ are two independent normal distributions, $\lambda_1X_1$ and $\lambda_2X_2$ are also independent, thus $e^{tY_1}$ and $e^{tY_2}$ are independent, then
$$m_Y(t)=E[e^{tY_1}]\cdot E[e^{tY_2}]=m_{Y_1}(t)m_{Y_2}(t)=e^{\mu_1\lambda_1t+\sigma_1^2\lambda_1^2t^2/2}\cdot e^{\mu_2\lambda_2t+\sigma_2^2\lambda_2^2t^2/2}=e^{(\mu_1\lambda_1+\mu_2\lambda_2)t+(\sigma_1^2\lambda_1^2+\sigma_2^2\lambda_2^2)t^2/2}.$$

According to the uniqueness of moment generating function and Theorem 1.3.10, we can conclude that $Y=\lambda_1X_1+\lambda_2X_2$ follows a normal distribution with mean $\mu_1\lambda_1+\mu_2\lambda_2$ and varaince $\sigma_1^2\lambda_1^2+\sigma_2^2\lambda_2^2$.

\subsection{Bivariate Normal Distribution}
First we can standardize $X_1$ and $X_2$ by
$$X=\frac{X_1-\mu_1}{\sigma_1},Y=\frac{X_2-\mu_2}{\sigma_2},$$
so that $X=\varphi_1\circ X_1$, $Y=\varphi_2\circ X_2$, where 
$$\varphi_1:R\mapsto R,\quad \varphi_1(x)=\frac{x-\mu_1}{\sigma_1},$$
$$\varphi_2:R\mapsto R,\quad \varphi_2(y)=\frac{y-\mu_2}{\sigma_2},$$
according to Theorem 1.3.13, and since $\varphi_1$ and $\varphi_2$ are single variables maps,
\begin{align*}
f_{XY}(x,y)&=f_{X_1X_2}(\varphi_1^{-1}(x),\varphi_2^{-2}(y))\cdot\left|\frac{d\varphi_1^{-1}(x)}{dx}\right|\cdot\left|\frac{d\varphi_2^{-1}(y)}{dy}\right|\\
&=\frac{1}{2\pi\sqrt{1-\varrho^2}}\exp\left[-\frac{x^2-2\varrho xy+y^2}{2(1-\varrho^2)}\right]\\
&=\frac{1}{2\pi\sqrt{1-\varrho^2}}\exp\left[-\frac{x^2(1-\varrho^2)+(\varrho x-y)^2}{2(1-\varrho^2)}\right].
\end{align*}

\begin{enumerate}[label=\roman*)]
\item
\begin{align*}
f_X(x)&=\int_{-\infty}^\infty f_{XY}(x,y)dy\\
&=\frac{1}{2\pi\sqrt{1-\varrho^2}}\int_{-\infty}^\infty \exp\left[-\frac{x^2-2\varrho xy+y^2}{2(1-\varrho^2)}\right] dy\\
&=\frac{e^{-x^2/2}}{2\pi\sqrt{1-\varrho^2}}\int_{-\infty}^\infty \exp\left[-\frac{(\varrho x-y)^2}{2(1-\varrho^2)}\right] dy\\
&=\frac{e^{-x^2/2}}{2\pi\sqrt{1-\varrho^2}}\int_{-\infty}^\infty \exp\left[-\frac{(y-\varrho x)^2}{2(1-\varrho^2)}\right] d(y-\varrho x)\\
&=\frac{e^{-x^2/2}}{2\pi\sqrt{1-\varrho^2}}\cdot\sqrt{2\pi(1-\varrho^2)}\\
&=\frac{1}{\sqrt{2\pi}}e^{-x^2/2}.
\end{align*}
So $f_X(x)$ follows a standard normal distribution, which means the marginal density $f_{X_1}(x_1)$ follows a normal distribution with mean $\mu_1$ and variance $\sigma_1^2$.
\item
\begin{align*}
&E[XY]=\int_{-\infty}^\infty\int_{-\infty}^\infty xy\cdot f_{XY}(x,y) dydx\\
=&\frac{1}{2\pi\sqrt{1-\varrho^2}}\int_{-\infty}^\infty\int_{-\infty}^\infty xy\cdot\exp\left[-\frac{x^2-2\varrho xy+y^2}{2(1-\varrho^2)}\right] dy dx\\
=&\frac{1}{2\pi\sqrt{1-\varrho^2}}\int_{-\infty}^\infty x\cdot e^{-x^2/2} \int_{-\infty}^\infty y\cdot\exp\left[-\frac{(\varrho x-y)^2}{2(1-\varrho^2)}\right] dy dx\\
=&\frac{1}{2\pi\sqrt{1-\varrho^2}}\int_{-\infty}^\infty x\cdot e^{-x^2/2} \int_{-\infty}^\infty (y-\varrho x+\varrho x)\cdot\exp\left[-\frac{(\varrho x-y)^2}{2(1-\varrho^2)}\right] dy dx\\
=&\frac{1}{2\pi\sqrt{1-\varrho^2}}\int_{-\infty}^\infty x\cdot e^{-x^2/2} \left\{\int_0^\infty \frac{1}{2}\cdot\exp\left[-\frac{(y-\varrho x)^2}{2(1-\varrho^2)}\right] d((y-\varrho x)^2)+\varrho x\cdot\sqrt{2\pi(1-\varrho^2)}\right\} dx\\
=&\frac{1}{2\pi\sqrt{1-\varrho^2}}\int_{-\infty}^\infty x\cdot e^{-x^2/2} \left[(1-\varrho^2)+\varrho x\cdot\sqrt{2\pi(1-\varrho^2)}\right] dx\\
=&\frac{1}{2\pi\sqrt{1-\varrho^2}}\cdot\varrho\sqrt{2\pi(1-\varrho^2)}\int_{-\infty}^\infty x^2\cdot e^{-x^2/2} dx\\
=&\varrho.
\end{align*}
\item
If $X_1$ and $X_2$ are independent, then ${\rm Cov}(X_1,X_2)=\varrho\sigma_1\sigma_2=0$, so $\varrho=0$.

If $\varrho=0$,
$$f_{XY}(x,y)=\frac{1}{2\pi}\exp\left[-\frac{x^2+y^2}{2}\right]=\frac{1}{\sqrt{2\pi}}e^{-x^2/2}\cdot\frac{1}{\sqrt{2\pi}}e^{-y^2/2}=f_X(x)\cdot f_Y(y),$$
so $X$ and $Y$ are independent, which means $X_1$ and $X_2$ are also independent.

In conclusion, $X_1$ and $X_2$ are independent if and only if $\varrho=0$.

For a bivariate random variable with an arbitrary distribution, it's not true. Consider a random distribution with three outcomes $(x,y)$: (−1, 1), (0, −2) and (1, 1) all with probability of $1/3$, then 
$${\rm Cov}(X,Y)=E((X-\mu_x)(Y-\mu_y))=\frac{1}{3}(-1+0+1)=0,$$
$$\varrho=\frac{{\rm Cov}(X,Y)}{\sigma_x\sigma_y}=0.$$
However, in the example there is a bijection between $X$ and $Y$, they are not independent.
\item

\end{enumerate}

\end{document}



\documentclass[11pt,a4paper]{article}

\usepackage{../ve401}
\usepackage{graphicx}

\author{Group 37}
\semester{Spring}
\year{2019}
\subtitle{Assignment}
\subtitlenumber{3}
\blockinfo{
	\bigskip
	\begin{center}
		\textbf{Group members}
	\end{center}
	\begin{itemize}\itemsep .25cm
		\item \href{mailto:hcm_9809@sjtu.edu.cn}{Chenmin Hou} (517370910248)
		\item \href{mailto:liuyh615@126.com}{Yihao Liu} (515370910207)
		\item \href{mailto:lyz0123@sjtu.edu.cn}{Yuzhou Li} (517021910922)
	\end{itemize}
}

\begin{document}

\maketitle

\subsection{}

\begin{enumerate}[label=\roman*)]
\item
From the definition of $f_{XY}(x,y)$, we know $f_{XY}(x,y)\geqslant0$ for all $(x,y)\in\Omega$, and
$$\sum_{(x,y)\in\Omega}f_{XY}(x,y)=\sum_{x=1}^n\sum_{y=x}^n\frac{2}{n(n+1)}=\sum_{x=1}^nx\frac{2}{n(n+1)}=\frac{n(n+1)}{2}\cdot\frac{2}{n(n+1)}=1.$$
So $f_{XY}$ is in fact a density.
\item
\begin{align*}
f_X(x)&=\sum_yf_{XY}(x,y)=\sum_{y=x}^n\frac{2}{n(n+1)}=\frac{2(n+1-x)}{n(n+1)},\\
f_Y(y)&=\sum_xf_{XY}(x,y)=\sum_{x=1}^y\frac{2}{n(n+1)}=\frac{2y}{n(n+1)}.
\end{align*}
\item
$$f_X(x)f_Y(y)=\frac{4(n+1-x)y}{n^2(n+1)^2},$$
$$f_{XY}(n,1)=\frac{2}{n(n+1)}\neq f_X(n)f_Y(1)=\frac{4}{n^2(n+1)^2}.$$
So $X$ and $Y$ are not independent.
\item
\begin{align*}
P[X\leqslant3{\rm\ and\ }Y\leqslant2]&=\sum_{x=1}^3\sum_{y=x}^2f_{XY}(x,y)=\sum_{x=1}^3\sum_{y=x}^2\frac{2}{n(n+1)}=3\cdot\frac{2}{5\cdot(5+1)}=\frac{1}{5},\\
P[X\leqslant3]&=\sum_{x=1}^3f_X(x)=\sum_{x=1}^3\frac{2(n+1-x)}{n(n+1)}=(5+4+3)\cdot\frac{2}{5\cdot(5+1)}=\frac{4}{5},\\
P[Y\leqslant2]&=\sum_{y=1}^2f_Y(y)=\sum_{y=1}^2\frac{2y}{n(n+1)}=(1+2)\cdot\frac{2}{5\cdot(5+1)}=\frac{1}{5}.
\end{align*}
\end{enumerate}

\subsection{The Sum of Two Continuous Random Variables}

Consider the transformation 
$$\varphi(X,Y)\mapsto (U,V)$$
where
$$\varphi(x,y)=\binom{x+y}{y}.$$
Then
$$\varphi^{-1}(u,v)=\binom{u-v}{v}.$$
We calculate
$$D\varphi^{-1}(u,v)=\begin{pmatrix}
\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\
\frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} \\
\end{pmatrix}=\begin{pmatrix}
1 & -1 \\ 0 & 1 \\
\end{pmatrix},$$
so
$$\left|\frac{\partial(x,y)}{\partial(u,v)}\right||\det D\varphi^{-1}(u,v)|=1.$$
Then
$$f_{UV}(u,v)=f_{XY}(x,y)|\det D\varphi^{-1}(u,v)|=f_{XY}(x,y).$$
The marginal density $f_U$ is given by
$$f_U(u)=\int_{-\infty}^\infty f_{UV}(u,v)dv=\int_{-\infty}^\infty f_{XY}(x,y)dy=\int_{-\infty}^\infty f_{XY}(u-v,v)dv.$$

\subsection{The Sum of Two Exponential Distributions}
Let
$$f_X(x)=\left\{\begin{aligned}
&\beta_1e^{-\beta_1x}=\frac{1}{3}e^{-x/3} &\quad x>0 \\
&0 &\quad x\leqslant 0
\end{aligned}\right.,\quad
f_Y(y)=\left\{\begin{aligned}
&\beta_2e^{-\beta_2y}=e^{-y} &\quad x>0 \\
&0 &\quad x\leqslant 0
\end{aligned}\right.,$$
and
$$f_{XY}(x,y)=f_X(x)f_Y(y)=\left\{\begin{aligned}
&\frac{1}{3}e^{-x/3-y} &\quad x>0 \\
&0 &\quad x\leqslant 0
\end{aligned}\right..$$
According to Exercise 3.2,
$$f_U(u)=\int_{-\infty}^\infty f_{XY}(u-v,v)dv.$$
When $u,v>0$, $u-v>0$, we can get $0<v<u$, then
$$f_U(u)=\int_0^u\frac{1}{3}e^{-(u-v)/3-v}dv=\frac{1}{3}e^{-u/3}\int_0^u e^{-2v/3}dv=\frac{1}{3}e^{-u/3}\cdot-\frac{3}{2}e^{-2v/3}\bigg|_0^u=\frac{1}{2}(e^{-u/3}-e^{-u}).$$
When $u,v\leqslant0$, $f_U(u)=0$, so
$$f_U(u)=\left\{\begin{aligned}
&\frac{1}{2}(e^{-u/3}-e^{-u}) &\quad u>0 \\
&0 &\quad u\leqslant 0
\end{aligned}\right..$$

\subsection{The Linear Combination of Two Normal Distributions}

$$f_{X_1}(x)=\frac{1}{\sqrt{2\pi}\sigma_1}\exp\left[-\frac{1}{2}\left(\frac{x-\mu_1}{\sigma_1}\right)^2\right].$$
Let $Y_1=\lambda_1X_1$, so that $Y_1=\varphi\circ X_1$, where $\varphi:R\mapsto R$, $\varphi(x)=\lambda_1 x$, according to Theorem 1.3.13,
$$f_{Y_1}(y)=f_{X_1}(\varphi^{-1}(y))\cdot\left|\frac{d\varphi^{-1}(y)}{dy}\right|=\frac{1}{\lambda_1}f_{X_1}\left(\frac{y}{\lambda_1}\right)=\frac{1}{\sqrt{2\pi}\sigma_1\lambda_1}\exp\left[-\frac{1}{2}\left(\frac{y-\mu_1\lambda_1}{\sigma_1\lambda_1}\right)^2\right].$$
So $Y_1=\lambda_1X_1$ follows a normal distribution with mean $\mu_1\lambda_1$ and variance $\sigma_1^2\lambda_1^2$.

Similarly, $Y_2=\lambda_2X_2$ follows a normal distribution with mean $\mu_2\lambda_2$ and variance $\sigma_2^2\lambda_2^2$.

The moment generating function of $Y=\lambda_1X_1+\lambda_2X_2=Y_1+Y_2$ is
$$m_Y(t)=E[e^{tY}]=E[e^{t(Y_1+Y_2)}].$$

Since $X_1$ and $X_2$ are two independent normal distributions, $\lambda_1X_1$ and $\lambda_2X_2$ are also independent, thus $e^{tY_1}$ and $e^{tY_2}$ are independent, then
$$m_Y(t)=E[e^{tY_1}]\cdot E[e^{tY_2}]=m_{Y_1}(t)m_{Y_2}(t)=e^{\mu_1\lambda_1t+\sigma_1^2\lambda_1^2t^2/2}\cdot e^{\mu_2\lambda_2t+\sigma_2^2\lambda_2^2t^2/2}=e^{(\mu_1\lambda_1+\mu_2\lambda_2)t+(\sigma_1^2\lambda_1^2+\sigma_2^2\lambda_2^2)t^2/2}.$$

According to the uniqueness of moment generating function and Theorem 1.3.10, we can conclude that $Y=\lambda_1X_1+\lambda_2X_2$ follows a normal distribution with mean $\mu_1\lambda_1+\mu_2\lambda_2$ and varaince $\sigma_1^2\lambda_1^2+\sigma_2^2\lambda_2^2$.

\subsection{Bivariate Normal Distribution}
First we can standardize $X_1$ and $X_2$ by
$$X=\frac{X_1-\mu_1}{\sigma_1},Y=\frac{X_2-\mu_2}{\sigma_2},$$
so that $X=\varphi_1\circ X_1$, $Y=\varphi_2\circ X_2$, where 
$$\varphi_1:R\mapsto R,\quad \varphi_1(x_1)=\frac{x_1-\mu_1}{\sigma_1},\quad\varphi_1^{-1}(x)=\sigma_1x+\mu_1$$
$$\varphi_2:R\mapsto R,\quad \varphi_2(x_2)=\frac{x_2-\mu_2}{\sigma_2},\quad\varphi_2^{-1}(y)=\sigma_2y+\mu_2$$
according to Theorem 1.3.13, and since $\varphi_1$ and $\varphi_2$ are single variables maps,
\begin{align*}
f_{XY}(x,y)&=f_{X_1X_2}(\varphi_1^{-1}(x),\varphi_2^{-2}(y))\cdot\left|\frac{d\varphi_1^{-1}(x)}{dx}\right|\cdot\left|\frac{d\varphi_2^{-1}(y)}{dy}\right|\\
&=\frac{1}{2\pi\sqrt{1-\varrho^2}}\exp\left[-\frac{x^2-2\varrho xy+y^2}{2(1-\varrho^2)}\right]\\
&=\frac{1}{2\pi\sqrt{1-\varrho^2}}\exp\left[-\frac{x^2(1-\varrho^2)+(\varrho x-y)^2}{2(1-\varrho^2)}\right].
\end{align*}

\begin{enumerate}[label=\roman*)]
\item
\begin{align*}
f_X(x)&=\int_{-\infty}^\infty f_{XY}(x,y)dy\\
&=\frac{1}{2\pi\sqrt{1-\varrho^2}}\int_{-\infty}^\infty \exp\left[-\frac{x^2-2\varrho xy+y^2}{2(1-\varrho^2)}\right] dy\\
&=\frac{e^{-x^2/2}}{2\pi\sqrt{1-\varrho^2}}\int_{-\infty}^\infty \exp\left[-\frac{(\varrho x-y)^2}{2(1-\varrho^2)}\right] dy\\
&=\frac{e^{-x^2/2}}{2\pi\sqrt{1-\varrho^2}}\int_{-\infty}^\infty \exp\left[-\frac{(y-\varrho x)^2}{2(1-\varrho^2)}\right] d(y-\varrho x)\\
&=\frac{e^{-x^2/2}}{2\pi\sqrt{1-\varrho^2}}\cdot\sqrt{2\pi(1-\varrho^2)}\\
&=\frac{1}{\sqrt{2\pi}}e^{-x^2/2}.
\end{align*}
So $f_X(x)$ follows a standard normal distribution, which means the marginal density $f_{X_1}(x_1)$ follows a normal distribution with mean $\mu_1$ and variance $\sigma_1^2$.
\item
\begin{align*}
&E[XY]=\int_{-\infty}^\infty\int_{-\infty}^\infty xy\cdot f_{XY}(x,y) dydx\\
=&\frac{1}{2\pi\sqrt{1-\varrho^2}}\int_{-\infty}^\infty\int_{-\infty}^\infty xy\cdot\exp\left[-\frac{x^2-2\varrho xy+y^2}{2(1-\varrho^2)}\right] dy dx\\
=&\frac{1}{2\pi\sqrt{1-\varrho^2}}\int_{-\infty}^\infty x\cdot e^{-x^2/2} \int_{-\infty}^\infty y\cdot\exp\left[-\frac{(\varrho x-y)^2}{2(1-\varrho^2)}\right] dy dx\\
=&\frac{1}{2\pi\sqrt{1-\varrho^2}}\int_{-\infty}^\infty x\cdot e^{-x^2/2} \int_{-\infty}^\infty (y-\varrho x+\varrho x)\cdot\exp\left[-\frac{(\varrho x-y)^2}{2(1-\varrho^2)}\right] dy dx\\
=&\frac{1}{2\pi\sqrt{1-\varrho^2}}\int_{-\infty}^\infty x\cdot e^{-x^2/2} \left\{\int_0^\infty \frac{1}{2}\cdot\exp\left[-\frac{(y-\varrho x)^2}{2(1-\varrho^2)}\right] d((y-\varrho x)^2)+\varrho x\cdot\sqrt{2\pi(1-\varrho^2)}\right\} dx\\
=&\frac{1}{2\pi\sqrt{1-\varrho^2}}\int_{-\infty}^\infty x\cdot e^{-x^2/2} \left[(1-\varrho^2)+\varrho x\cdot\sqrt{2\pi(1-\varrho^2)}\right] dx\\
=&\frac{1}{2\pi\sqrt{1-\varrho^2}}\cdot\varrho\sqrt{2\pi(1-\varrho^2)}\int_{-\infty}^\infty x^2\cdot e^{-x^2/2} dx\\
=&\varrho.
\end{align*}
\item
If $X_1$ and $X_2$ are independent, then ${\rm Cov}(X_1,X_2)=\varrho\sigma_1\sigma_2=0$, so $\varrho=0$.

If $\varrho=0$,
$$f_{XY}(x,y)=\frac{1}{2\pi}\exp\left[-\frac{x^2+y^2}{2}\right]=\frac{1}{\sqrt{2\pi}}e^{-x^2/2}\cdot\frac{1}{\sqrt{2\pi}}e^{-y^2/2}=f_X(x)\cdot f_Y(y),$$
so $X$ and $Y$ are independent, which means $X_1$ and $X_2$ are also independent.

In conclusion, $X_1$ and $X_2$ are independent if and only if $\varrho=0$.

For a bivariate random variable with an arbitrary distribution, it's not true. Consider a random distribution with three outcomes $(x,y)$: (−1, 1), (0, −2) and (1, 1) all with probability of $1/3$, then 
$${\rm Cov}(X,Y)=E((X-\mu_x)(Y-\mu_y))=\frac{1}{3}(-1+0+1)=0,$$
$$\varrho=\frac{{\rm Cov}(X,Y)}{\sigma_x\sigma_y}=0.$$
However, in the example there is a bijection between $X$ and $Y$, they are not independent.
\item
$$f_{Y|x}(y)=\frac{f_{XY}(x,y)}{f_X(x)}=\frac{\frac{1}{2\pi\sqrt{1-\varrho^2}}\exp\left[-\frac{x^2-2\varrho xy+y^2}{2(1-\varrho^2)}\right]}{\frac{1}{\sqrt{2\pi}}e^{-x^2/2}}=\frac{1}{\sqrt{2\pi}\sqrt{1-\varrho^2}}\exp\left[-\frac{1}{2}\left(\frac{y-\varrho x}{\sqrt{1-\varrho^2}}\right)^2\right].$$
So $f_{Y|x}(y)$ follows a normal distribution with mean $\varrho x$ and variance $\sqrt{1-\varrho^2}$, $\mu_{Y|x}=\varrho x$. Then we can apply a inverse transformation with $\varphi_1$ and $\varphi_2$,
$$f_{X_2|x_1}(x_2)=f_{Y|x}(\varphi_2(x_2))\cdot\left|\frac{d\varphi_2(x_2)}{dx_2}\right|=\frac{1}{\sqrt{2\pi}\sigma_2\sqrt{1-\varrho^2}}\exp\left[-\frac{1}{2}\left(\frac{x_2-\mu_2-\varrho\frac{\sigma_2}{\sigma_1}(x_1-\mu_1)}{\sigma_2\sqrt{1-\varrho^2}}\right)^2\right].$$
So $f_{X_2|x_1}(x_2)$ follows a normal distribution with mean $\mu_2+\varrho\frac{\sigma_2}{\sigma_1}(x_1-\mu_1)$ and variance $\sigma_2\sqrt{1-\varrho^2}$,
$$\mu_{X_2|x_1}=\mu_2+\varrho\frac{\sigma_2}{\sigma_1}(x_1-\mu_1).$$
\item
According to the symmetric of $X_1$ and $X_2$, $f_{X_1|x_2}(x_1)$ follows a normal distribution with mean $\mu_1+\varrho\frac{\sigma_1}{\sigma_2}(x_2-\mu_2)$ and variance $\sigma_1\sqrt{1-\varrho^2}$, so
$$\mu_{X_1|x_2}=\mu_1+\varrho\frac{\sigma_1}{\sigma_2}(x_2-\mu_2)=2000+0.87\cdot\frac{\sqrt{2500}}{\sqrt{0.01}}\cdot(0.098-0.1)=1999.19{\rm~inch},$$
$$\sigma_{X_1|x_2}=\sigma_1\sqrt{1-\varrho^2}=\sqrt{2500}\cdot\sqrt{1-0.87^2}\approx24.65{\rm~inch},$$
$$P[X_1|x_2\geqslant1950]=P\left[Z\geqslant\frac{X_1|x_2-\mu_{X_1|x_2}}{\sigma_{X_1|x_2}}\right]\approx P\left[Z\geqslant\frac{1950-1999.19}{24.65}\right]\approx 1-\Phi(-1.996)\approx0.977.$$
\end{enumerate}

\subsection{Bivariate Normal Distribution as a Mixture of Independent Normal Distributions}

Let $$A=\begin{pmatrix}a_1 & b_1 \\a_2 & b_2\end{pmatrix},$$
then $$Y=AX=\begin{pmatrix}
a_1X_1 + b_1X_2 \\
a_2X_1 + b_2X_2
\end{pmatrix}.$$

\begin{enumerate}[label=\roman*)]
\item
$$E[AX]=\begin{pmatrix}
E[a_1X_1 + b_1X_2]\\
E[a_2X_1 + b_2X_2]
\end{pmatrix}=\begin{pmatrix}
a_1E[X_1] + b_1E[X_2]\\
a_2E[X_1] + b_2E[X_2]
\end{pmatrix}=AE[X].$$
\item
Let
\begin{align*}
M&={\rm Var}[a_1X_1 + b_1X_2]=a_1^2{\rm Var}[X_1]+2a_1b_1{\rm Cov}(X_1,X_2)+b_1^2{\rm Var}[X_2],\\
N&={\rm Cov}(a_1X_1 + b_1X_2,a_2X_1 + b_2X_2)\\
&=E[(a_1X_1 + b_1X_2)(a_2X_1 + b_2X_2)]-E[a_1X_1 + b_1X_2]E[a_2X_1 + b_2X_2]\\
&=a_1a_2E[X_1^2]+(a_1b_2+a_2b_1)E[X_1X_2]+b_1b_2E[X_2^2]\\
&\quad-a_1a_2E[X_1]^2-(a_1b_2+a_2b_1)E[X_1]E[X_2]-b_1b_2E[X_2]^2\\
&=a_1a_2{\rm Var}[X_1]+(a_1b_2+a_2b_1){\rm Cov}(X_1,X_2)+b_1b_2{\rm Var}[X_2].\\
P&={\rm Var}[a_2X_1 + b_2X_2]=a_2^2{\rm Var}[X_1]+2a_2b_2{\rm Cov}(X_1,X_2)+b_2^2{\rm Var}[X_2].
\end{align*}
\begin{align*}
{\rm Var}[AX]&=\begin{pmatrix}
{\rm Var}[a_1X_1 + b_1X_2] & {\rm Cov}(a_1X_1 + b_1X_2,a_2X_1 + b_2X_2)\\
{\rm Cov}(a_2X_1 + b_2X_2,a_1X_1 + b_1X_2) & {\rm Var}[a_2X_1 + b_2X_2]
\end{pmatrix}\\
&=\begin{pmatrix}
M & N \\ N & P
\end{pmatrix}\\
&=\begin{pmatrix}a_1 & b_1 \\a_2 & b_2\end{pmatrix}
\begin{pmatrix}{\rm Var}[X_1] & {\rm Cov}(X_1,X_2) \\ {\rm Cov}(X_2,X_1) & {\rm Var}[X_2] \end{pmatrix}
\begin{pmatrix}a_1 & a_2 \\b_1 & b_2\end{pmatrix}\\
&=A{\rm Var}[X]A^T,
\end{align*}
\item
Since
$$\Sigma_X=\begin{pmatrix}
\sigma_1^2 & 0 \\ 0 & \sigma_2^2
\end{pmatrix},$$
we can get
$$\Sigma^{-1}_X=\begin{pmatrix}
\frac{1}{\sigma_1^2} & 0 \\ 0 & \frac{1}{\sigma_2^2}
\end{pmatrix},\quad \det\Sigma_X=\sigma_1^2\sigma_2^2,$$
$$\left\langle x-\mu_x, \Sigma^{-1}_X (x-\mu_x) \right\rangle=
\begin{pmatrix}x_1-\mu_1 & x_2-\mu_2\end{pmatrix}
\left[\begin{pmatrix}\frac{1}{\sigma_1^2} & 0 \\ 0 & \frac{1}{\sigma_2^2}\end{pmatrix}
\begin{pmatrix}x_1-\mu_1 \\ x_2-\mu_2\end{pmatrix}\right]
=\frac{(x_1-\mu_1)^2}{\sigma_1^2}+\frac{(x_2-\mu_2)^2}{\sigma_2^2}.
$$
And since $X_1$ and $X_2$ are independent,
\begin{align*}
f_X(x)&=f_X(x_1,x_2)=f_{X_1}(x_1)f_{X_2}(x_2)\\
&=\frac{1}{2\pi\sigma_1\sigma_2}\exp\left[-\frac{1}{2}\left(\frac{(x_1-\mu_1)^2}{\sigma_1^2}+\frac{(x_2-\mu_2)^2}{\sigma_2^2}\right)\right]\\
&=\frac{1}{2\pi\sqrt{\det\Sigma_X}}\exp\left[-\frac{1}{2}\left\langle x-\mu_x, \Sigma^{-1}_X (x-\mu_x) \right\rangle\right].
\end{align*}
\item
According to i) and ii),
$$\mu_y=A\mu_x,\quad \Sigma_Y={\rm Var}[Y]=A{\rm Var}[X]A^T=A\Sigma_XA^T.$$
Then
$$\det\Sigma_Y=\det(A\Sigma_XA^T)=(\det A)^2\det\Sigma_X,$$
$$\Sigma_Y^{-1}=(A^{-1})^T\Sigma_X^{-1}A^{-1},$$
\begin{align*}
\left\langle x-\mu_x, \Sigma^{-1}_X (x-\mu_x) \right\rangle
&=\left\langle A^{-1}y-A^{-1}\mu_y, \Sigma^{-1}_X (A^{-1}y-A^{-1}\mu_y) \right\rangle\\
&=(A^{-1}(y-\mu_y))^T\Sigma^{-1}_X A^{-1}(y-\mu_y)\\
&=(y-\mu_y)^T(A^{-1})^T\Sigma^{-1}_X A^{-1}(y-\mu_y)\\
&=\left\langle y-\mu_y, \Sigma^{-1}_Y (y-\mu_y) \right\rangle.
\end{align*}
With the inverse transformation of $A$, we obtain
\begin{align*}
f_Y(y)&=f_X(A^{-1}y)|\det A^{-1}|\\
&=\frac{1}{2\pi\sqrt{\det\Sigma_X}}\exp\left[-\frac{1}{2}\left\langle x-\mu_x, \Sigma^{-1}_X (x-\mu_x) \right\rangle\right]\cdot\frac{1}{|\det A|}\\
&=\frac{1}{2\pi\sqrt{\det\Sigma_X(\det A)^2}}\exp\left[-\frac{1}{2}\left\langle A^{-1}y-A^{-1}\mu_y, \Sigma^{-1}_X (A^{-1}y-A^{-1}\mu_y) \right\rangle\right]\\
&=\frac{1}{2\pi\sqrt{|\det\Sigma_Y}|}\exp\left[-\frac{1}{2}\left\langle y-\mu_y, \Sigma^{-1}_Y (y-\mu_y) \right\rangle\right].
\end{align*}
\item
$$|\det\Sigma_Y|=|{\rm Var}[Y_1]{\rm Var}[Y_2]-{\rm Cov}[Y_1,Y_2]|^2=|\sigma_{Y_1}^2\sigma_{Y_2}^2-{\rm Cov}[Y_1,Y_2]^2|,$$
$$\sqrt{|\det\Sigma_Y|}=\sigma_{Y_1}\sigma_{Y_2}\sqrt{1-\frac{{\rm Cov}[Y_1,Y_2]^2}{\sigma_{Y_1}^2\sigma_{Y_2}^2}}=\sigma_{Y_1}\sigma_{Y_2}\sqrt{1-\varrho^2}.$$
Then we can write
$$\Sigma_Y=\begin{pmatrix}
\sigma_{Y_1}^2 & \varrho\sigma_{Y_1}\sigma_{Y_2} \\
\varrho\sigma_{Y_1}\sigma_{Y_2} & \sigma_{Y_2}^2
\end{pmatrix},\quad\Sigma_Y^{-1}=\frac{1}{\sqrt{1-\varrho^2}}\begin{pmatrix}
\frac{1}{\sigma_{Y_1}^2} & \frac{\varrho}{\sigma_{Y_1}\sigma_{Y_2}} \\
\frac{\varrho}{\sigma_{Y_1}\sigma_{Y_2}} & \frac{1}{\sigma_{Y_2}^2}
\end{pmatrix},$$
and
\begin{align*}
&\left\langle y-\mu_y, \Sigma^{-1}_Y (y-\mu_y) \right\rangle\\
=&\frac{1}{\sqrt{1-\varrho^2}}
\begin{pmatrix}y_1-\mu_{Y_1} & y_2-\mu_{Y_2}\end{pmatrix}
\left[\begin{pmatrix}
\frac{1}{\sigma_{Y_1}^2} & \frac{\varrho}{\sigma_{Y_1}\sigma_{Y_2}} \\
\frac{\varrho}{\sigma_{Y_1}\sigma_{Y_2}} & \frac{1}{\sigma_{Y_2}^2}
\end{pmatrix}
\begin{pmatrix}y_1-\mu_{Y_1} \\ y_2-\mu_{Y_2}\end{pmatrix}
\right]\\
=&\frac{1}{\sqrt{1-\varrho^2}}\left[\left(\frac{y_1-\mu_{Y_1}}{\sigma_{Y_1}}\right)^2-2\varrho\left(\frac{y_1-\mu_{Y_1}}{\sigma_{Y_1}}\right)\left(\frac{y_2-\mu_{Y_2}}{\sigma_{Y_2}}\right)+\left(\frac{y_2-\mu_{Y_2}}{\sigma_{Y_2}}\right)^2\right].
\end{align*}
So $f_Y(y)$ can be written as
\begin{footnotesize}
$$f_Y(y_1,y_2)=\frac{1}{2\pi\sigma_{Y_1}\sigma_{Y_2}\sqrt{1-\varrho^2}}\exp\left\{-\frac{1}{2(1-\varrho^2)}\left[\left(\frac{y_1-\mu_{Y_1}}{\sigma_{Y_1}}\right)^2-2\varrho\left(\frac{y_1-\mu_{Y_1}}{\sigma_{Y_1}}\right)\left(\frac{y_2-\mu_{Y_2}}{\sigma_{Y_2}}\right)+\left(\frac{y_2-\mu_{Y_2}}{\sigma_{Y_2}}\right)^2\right]\right\}.$$
\end{footnotesize}
\end{enumerate}

\subsection{}

\includegraphics[width=\linewidth]{ex3_7.pdf}

\begin{enumerate}[label=\roman*)]
\item\ \\
\includegraphics[width=0.8\linewidth]{ex3_7_1.pdf}

We can find that the shape of the stem-and-leaf graph seems like a normal  distribution.
\item\ \\
\includegraphics[width=0.8\linewidth]{ex3_7_2.pdf}

The shape of the histogram seems like a normal distribution, which is the same as the stem-and-leaf graph shows.
\item\ \\
\includegraphics[width=\linewidth]{ex3_7_3.pdf}

We can find only one near outlier in the box plot, which shows that the data is very probably follow a normal distribution. It gives a more precise descriptive statistics than the stem-and-leaf graph.
\end{enumerate}

\subsection{}
$$L(\gamma)=\prod_{i=1}^nf(x_i)=\prod_{i=1}^n(\gamma+1)X_i^\gamma=(\gamma+1)^n\prod_{i=1}^nX_i^\gamma.$$
We want to find the value of $\gamma$ to maximize $L(\gamma)$, first we can take the logarithm on both sides, so that
$$\ln L(\gamma)=n\ln(\gamma+1)+\gamma\sum_{i=1}^n\ln X_i.$$
Maximizing $\ln L(\gamma)$ will also maximize $L(\gamma)$, so we take the first derivative and set it equal to zero:
$$\frac{d\ln L(\gamma)}{d\gamma}=\frac{n}{\gamma+1}+\sum_{i=1}^n\ln X_i=0.$$
The maximum likelihood estimator for $\gamma$ is
$$\gamma=-\frac{n}{\sum_{i=1}^n\ln X_i}-1.$$

\subsection{}

$$f_X(x)=\binom{n}{x}p^x(1-p)^{n-x}=\binom{4}{x}p^x(1-p)^{4-x}.$$
\begin{enumerate}[label=\roman*)]
\item
$$L(p)=\prod_i^nf_X(x_i)=f_X(0)^8\cdot f_X(1)^6\cdot f_X(2).$$
$$f_X(0)=(1-p)^4,\quad f_X(1)=4p(1-p)^3,\quad f_X(2)=6p^2(1-p)^2.$$
$$L(p)=(1-p)^{32}\cdot4^6p^6(1-p)^{18}\cdot6p^2(1-p)^2=6\cdot4^6p^8(1-p)^{52}.$$
We want to find the value of $p$ to maximize $L(p)$, first we can take the logarithm on both sides, so that
$$\ln L(p)=6\ln4+\ln6+8\ln p+52\ln (1-p).$$
Maximizing $\ln L(p)$ will also maximize $L(p)$, so we take the first derivative and set it equal to zero:
$$\frac{d\ln L(p)}{dp}=\frac{8}{p}-\frac{52}{1-p}=0,$$
$$8-8p=52p\Longrightarrow p=\frac{2}{15}.$$
The maximum likelihood estimate for $p$ is $2/15$.
\item
Since $2/15\approx13.3\%>10\%$, I would have some doubts concerning the use of this new material.
\end{enumerate}

\subsection{}
According to Exercise 3.6, we know
$$E[Y]=E[AX]=AE[X],\quad {\rm Var}[Y]={\rm Var}[AX]=A{\rm Var}[X]A^T.$$
Since $X_1$ and $X_2$ both follow normal distributions with variance $\sigma^2$ and mean $\mu$,
$${\rm Var}[X]=\begin{pmatrix}\sigma^2&0\\0&\sigma^2\end{pmatrix}=\sigma^2\begin{pmatrix}1&0\\0&1\end{pmatrix}=\sigma^2I_2.$$
And since $A^T=A^{-1}$,
$${\rm Var}[Y]=A{\rm Var}[X]A^T=A{\rm Var}[X]A^{-1}={\rm Var}[X],$$
$${\rm Cov}(Y_1,Y_2)={\rm Cov}(X_1,X_2).$$
According to Exercise 3.4, $Y_1$ and $Y_2$ follow normal distributions, and according to Exercise 3.5 iii), $Y_1$ and $Y_2$ are independent. The mean of $Y$ is $AE[X]$ and the variance of $Y$ is $\sigma^2 I_2$. 

\subsection{}

The length of the interval, $\sigma(z_{\alpha_1}+z_{\alpha_2})/\sqrt{n}$ is minimized if and only if $z_{\alpha_1}+z_{\alpha_2}$ is minimized. 

By the definition of $z$ we know
$$\phi(x)=\frac{1}{\sqrt{2\pi}}e^{-x^2/2},$$
$$\int_{z_{\alpha/2}}^{z_{\alpha_1}}\phi(x)dx=\frac{\alpha}{2}-\alpha_1=\alpha_2-\frac{\alpha}{2}=\int^{z_{\alpha/2}}_{z_{\alpha_2}}\phi(x)dx.$$
Without loss of generality, suppose $\alpha_1<\alpha/2<\alpha_2$, so that $$0<z_{\alpha_2}<z_{\alpha/2}<z_{\alpha_1}.$$
Since $\phi(x)$ is a decreasing function, when $0<z_{\alpha_2}<z_{\alpha/2}<z_{\alpha_1}$, 
$$\int_{z_{\alpha/2}}^{z_{\alpha_1}}\phi(x)dx<(z_{\alpha_1}-z_{\alpha/2})\phi(z_{\alpha/2}),$$
$$\int^{z_{\alpha/2}}_{z_{\alpha_2}}\phi(x)dx>(z_{\alpha/2}-z_{\alpha_2})\phi(z_{\alpha/2}).$$
So $$z_{\alpha/2}-z_{\alpha_2}<z_{\alpha_1}-z_{\alpha/2},$$
$$z_{\alpha_1}+z_{\alpha_2}>2z_{\alpha/2},$$
which means only when $\alpha_1=\alpha_2=\alpha/2$, $z_{\alpha_1}+z_{\alpha_2}$ is minimized.
\end{document}


\documentclass[11pt,a4paper]{article}

\usepackage{../ve401}

\author{Group 37}
\semester{Spring}
\year{2019}
\subtitle{Assignment}
\subtitlenumber{3}
\blockinfo{
	\bigskip
	\begin{center}
		\textbf{Group members}
	\end{center}
	\begin{itemize}\itemsep .25cm
		\item \href{mailto:hcm_9809@sjtu.edu.cn}{Chenmin Hou} (517370910248)
		\item \href{mailto:liuyh615@126.com}{Yihao Liu} (515370910207)
		\item \href{mailto:lyz0123@sjtu.edu.cn}{Yuzhou Li} (517021910922)
	\end{itemize}
}

\begin{document}

\maketitle

\subsection{}

\begin{enumerate}[label=\roman*)]
\item
From the definition of $f_{XY}(x,y)$, we know $f_{XY}(x,y)\geqslant0$ for all $(x,y)\in\Omega$, and
$$\sum_{(x,y)\in\Omega}f_{XY}(x,y)=\sum_{x=1}^n\sum_{y=x}^n\frac{2}{n(n+1)}=\sum_{x=1}^nx\frac{2}{n(n+1)}=\frac{n(n+1)}{2}\cdot\frac{2}{n(n+1)}=1.$$
So $f_{XY}$ is in fact a density.
\item
\begin{align*}
f_X(x)&=\sum_yf_{XY}(x,y)=\sum_{y=x}^n\frac{2}{n(n+1)}=\frac{2(n+1-x)}{n(n+1)},\\
f_Y(y)&=\sum_xf_{XY}(x,y)=\sum_{x=1}^y\frac{2}{n(n+1)}=\frac{2y}{n(n+1)}.
\end{align*}
\item
$$f_X(x)f_Y(y)=\frac{4(n+1-x)y}{n^2(n+1)^2},$$
$$f_{XY}(n,1)=\frac{2}{n(n+1)}\neq f_X(n)f_Y(1)=\frac{4}{n^2(n+1)^2}.$$
So $X$ and $Y$ are not independent.
\item
\begin{align*}
P[X\leqslant3{\rm\ and\ }Y\leqslant2]&=\sum_{x=1}^3\sum_{y=x}^2f_{XY}(x,y)=\sum_{x=1}^3\sum_{y=x}^2\frac{2}{n(n+1)}=3\cdot\frac{2}{5\cdot(5+1)}=\frac{1}{5},\\
P[X\leqslant3]&=\sum_{x=1}^3f_X(x)=\sum_{x=1}^3\frac{2(n+1-x)}{n(n+1)}=(5+4+3)\cdot\frac{2}{5\cdot(5+1)}=\frac{4}{5},\\
P[Y\leqslant2]&=\sum_{y=1}^2f_Y(y)=\sum_{y=1}^2\frac{2y}{n(n+1)}=(1+2)\cdot\frac{2}{5\cdot(5+1)}=\frac{1}{5}.
\end{align*}
\end{enumerate}

\subsection{The Sum of Two Continuous Random Variables}

Consider the transformation 
$$\varphi(X,Y)\mapsto (U,V)$$
where
$$\varphi(x,y)=\binom{x+y}{y}.$$
Then
$$\varphi^{-1}(u,v)=\binom{u-v}{v}.$$
We calculate
$$D\varphi^{-1}(u,v)=\begin{pmatrix}
\frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\
\frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} \\
\end{pmatrix}=\begin{pmatrix}
1 & -1 \\ 0 & 1 \\
\end{pmatrix},$$
so
$$\left|\frac{\partial(x,y)}{\partial(u,v)}\right||\det D\varphi^{-1}(u,v)|=1.$$
Then
$$f_{UV}(u,v)=f_{XY}(x,y)|\det D\varphi^{-1}(u,v)|=f_{XY}(x,y).$$
The marginal density $f_U$ is given by
$$f_U(u)=\int_{-\infty}^\infty f_{UV}(u,v)dv=\int_{-\infty}^\infty f_{XY}(x,y)dy=\int_{-\infty}^\infty f_{XY}(u-v,v)dv.$$

\subsection{The Sum of Two Exponential Distributions}
Let
$$f_X(x)=\left\{\begin{aligned}
&\beta_1e^{-\beta_1x}=\frac{1}{3}e^{-x/3} &\quad x>0 \\
&0 &\quad x\leqslant 0
\end{aligned}\right.,\quad
f_Y(y)=\left\{\begin{aligned}
&\beta_2e^{-\beta_2y}=e^{-y} &\quad x>0 \\
&0 &\quad x\leqslant 0
\end{aligned}\right.,$$
and
$$f_{XY}(x,y)=f_X(x)f_Y(y)=\left\{\begin{aligned}
&\frac{1}{3}e^{-x/3-y} &\quad x>0 \\
&0 &\quad x\leqslant 0
\end{aligned}\right..$$
According to Exercise 3.2,
$$f_U(u)=\int_{-\infty}^\infty f_{XY}(u-v,v)dv.$$
When $u,v>0$, $u-v>0$, we can get $0<v<u$, then
$$f_U(u)=\int_0^u\frac{1}{3}e^{-(u-v)/3-v}dv=\frac{1}{3}e^{-u/3}\int_0^u e^{-2v/3}dv=\frac{1}{3}e^{-u/3}\cdot-\frac{3}{2}e^{-2v/3}\bigg|_0^u=\frac{1}{2}(e^{-u/3}-e^{-u}).$$
When $u,v\leqslant0$, $f_U(u)=0$, so
$$f_U(u)=\left\{\begin{aligned}
&\frac{1}{2}(e^{-u/3}-e^{-u}) &\quad u>0 \\
&0 &\quad u\leqslant 0
\end{aligned}\right..$$

\subsection{The Linear Combination of Two Normal Distributions}

$$f_{X_1}(x)=\frac{1}{\sqrt{2\pi}\sigma_1}\exp\left[-\frac{1}{2}\left(\frac{x-\mu_1}{\sigma_1}\right)^2\right].$$
Let $Y_1=\lambda_1X_1$, so that $Y_1=\varphi\circ X_1$, where $\varphi:R\mapsto R$, $\varphi(x)=\lambda_1 x$, according to Theorem 1.3.13,
$$f_{Y_1}(y)=f_{X_1}(\varphi^{-1}(y))\cdot\left|\frac{d\varphi^{-1}(y)}{dy}\right|=\frac{1}{\lambda_1}f_{X_1}\left(\frac{y}{\lambda_1}\right)=\frac{1}{\sqrt{2\pi}\sigma_1\lambda_1}\exp\left[-\frac{1}{2}\left(\frac{y-\mu_1\lambda_1}{\sigma_1\lambda_1}\right)^2\right].$$
So $Y_1=\lambda_1X_1$ follows a normal distribution with mean $\mu_1\lambda_1$ and variance $\sigma_1^2\lambda_1^2$.

Similarly, $Y_2=\lambda_2X_2$ follows a normal distribution with mean $\mu_2\lambda_2$ and variance $\sigma_2^2\lambda_2^2$.

The moment generating function of $Y=\lambda_1X_1+\lambda_2X_2=Y_1+Y_2$ is
$$m_Y(t)=E[e^{tY}]=E[e^{t(Y_1+Y_2)}].$$

Since $X_1$ and $X_2$ are two independent normal distributions, $\lambda_1X_1$ and $\lambda_2X_2$ are also independent, thus $e^{tY_1}$ and $e^{tY_2}$ are independent, then
$$m_Y(t)=E[e^{tY_1}]\cdot E[e^{tY_2}]=m_{Y_1}(t)m_{Y_2}(t)=e^{\mu_1\lambda_1t+\sigma_1^2\lambda_1^2t^2/2}\cdot e^{\mu_2\lambda_2t+\sigma_2^2\lambda_2^2t^2/2}=e^{(\mu_1\lambda_1+\mu_2\lambda_2)t+(\sigma_1^2\lambda_1^2+\sigma_2^2\lambda_2^2)t^2/2}.$$

According to the uniqueness of moment generating function and Theorem 1.3.10, we can conclude that $Y=\lambda_1X_1+\lambda_2X_2$ follows a normal distribution with mean $\mu_1\lambda_1+\mu_2\lambda_2$ and varaince $\sigma_1^2\lambda_1^2+\sigma_2^2\lambda_2^2$.

\subsection{Bivariate Normal Distribution}
First we can standardize $X_1$ and $X_2$ by
$$X=\frac{X_1-\mu_1}{\sigma_1},Y=\frac{X_2-\mu_2}{\sigma_2},$$
so that $X=\varphi_1\circ X_1$, $Y=\varphi_2\circ X_2$, where 
$$\varphi_1:R\mapsto R,\quad \varphi_1(x_1)=\frac{x_1-\mu_1}{\sigma_1},\quad\varphi_1^{-1}(x)=\sigma_1x+\mu_1$$
$$\varphi_2:R\mapsto R,\quad \varphi_2(x_2)=\frac{x_2-\mu_2}{\sigma_2},\quad\varphi_2^{-1}(y)=\sigma_2y+\mu_2$$
according to Theorem 1.3.13, and since $\varphi_1$ and $\varphi_2$ are single variables maps,
\begin{align*}
f_{XY}(x,y)&=f_{X_1X_2}(\varphi_1^{-1}(x),\varphi_2^{-2}(y))\cdot\left|\frac{d\varphi_1^{-1}(x)}{dx}\right|\cdot\left|\frac{d\varphi_2^{-1}(y)}{dy}\right|\\
&=\frac{1}{2\pi\sqrt{1-\varrho^2}}\exp\left[-\frac{x^2-2\varrho xy+y^2}{2(1-\varrho^2)}\right]\\
&=\frac{1}{2\pi\sqrt{1-\varrho^2}}\exp\left[-\frac{x^2(1-\varrho^2)+(\varrho x-y)^2}{2(1-\varrho^2)}\right].
\end{align*}

\begin{enumerate}[label=\roman*)]
\item
\begin{align*}
f_X(x)&=\int_{-\infty}^\infty f_{XY}(x,y)dy\\
&=\frac{1}{2\pi\sqrt{1-\varrho^2}}\int_{-\infty}^\infty \exp\left[-\frac{x^2-2\varrho xy+y^2}{2(1-\varrho^2)}\right] dy\\
&=\frac{e^{-x^2/2}}{2\pi\sqrt{1-\varrho^2}}\int_{-\infty}^\infty \exp\left[-\frac{(\varrho x-y)^2}{2(1-\varrho^2)}\right] dy\\
&=\frac{e^{-x^2/2}}{2\pi\sqrt{1-\varrho^2}}\int_{-\infty}^\infty \exp\left[-\frac{(y-\varrho x)^2}{2(1-\varrho^2)}\right] d(y-\varrho x)\\
&=\frac{e^{-x^2/2}}{2\pi\sqrt{1-\varrho^2}}\cdot\sqrt{2\pi(1-\varrho^2)}\\
&=\frac{1}{\sqrt{2\pi}}e^{-x^2/2}.
\end{align*}
So $f_X(x)$ follows a standard normal distribution, which means the marginal density $f_{X_1}(x_1)$ follows a normal distribution with mean $\mu_1$ and variance $\sigma_1^2$.
\item
\begin{align*}
&E[XY]=\int_{-\infty}^\infty\int_{-\infty}^\infty xy\cdot f_{XY}(x,y) dydx\\
=&\frac{1}{2\pi\sqrt{1-\varrho^2}}\int_{-\infty}^\infty\int_{-\infty}^\infty xy\cdot\exp\left[-\frac{x^2-2\varrho xy+y^2}{2(1-\varrho^2)}\right] dy dx\\
=&\frac{1}{2\pi\sqrt{1-\varrho^2}}\int_{-\infty}^\infty x\cdot e^{-x^2/2} \int_{-\infty}^\infty y\cdot\exp\left[-\frac{(\varrho x-y)^2}{2(1-\varrho^2)}\right] dy dx\\
=&\frac{1}{2\pi\sqrt{1-\varrho^2}}\int_{-\infty}^\infty x\cdot e^{-x^2/2} \int_{-\infty}^\infty (y-\varrho x+\varrho x)\cdot\exp\left[-\frac{(\varrho x-y)^2}{2(1-\varrho^2)}\right] dy dx\\
=&\frac{1}{2\pi\sqrt{1-\varrho^2}}\int_{-\infty}^\infty x\cdot e^{-x^2/2} \left\{\int_0^\infty \frac{1}{2}\cdot\exp\left[-\frac{(y-\varrho x)^2}{2(1-\varrho^2)}\right] d((y-\varrho x)^2)+\varrho x\cdot\sqrt{2\pi(1-\varrho^2)}\right\} dx\\
=&\frac{1}{2\pi\sqrt{1-\varrho^2}}\int_{-\infty}^\infty x\cdot e^{-x^2/2} \left[(1-\varrho^2)+\varrho x\cdot\sqrt{2\pi(1-\varrho^2)}\right] dx\\
=&\frac{1}{2\pi\sqrt{1-\varrho^2}}\cdot\varrho\sqrt{2\pi(1-\varrho^2)}\int_{-\infty}^\infty x^2\cdot e^{-x^2/2} dx\\
=&\varrho.
\end{align*}
\item
If $X_1$ and $X_2$ are independent, then ${\rm Cov}(X_1,X_2)=\varrho\sigma_1\sigma_2=0$, so $\varrho=0$.

If $\varrho=0$,
$$f_{XY}(x,y)=\frac{1}{2\pi}\exp\left[-\frac{x^2+y^2}{2}\right]=\frac{1}{\sqrt{2\pi}}e^{-x^2/2}\cdot\frac{1}{\sqrt{2\pi}}e^{-y^2/2}=f_X(x)\cdot f_Y(y),$$
so $X$ and $Y$ are independent, which means $X_1$ and $X_2$ are also independent.

In conclusion, $X_1$ and $X_2$ are independent if and only if $\varrho=0$.

For a bivariate random variable with an arbitrary distribution, it's not true. Consider a random distribution with three outcomes $(x,y)$: (−1, 1), (0, −2) and (1, 1) all with probability of $1/3$, then 
$${\rm Cov}(X,Y)=E((X-\mu_x)(Y-\mu_y))=\frac{1}{3}(-1+0+1)=0,$$
$$\varrho=\frac{{\rm Cov}(X,Y)}{\sigma_x\sigma_y}=0.$$
However, in the example there is a bijection between $X$ and $Y$, they are not independent.
\item
$$f_{Y|x}(y)=\frac{f_{XY}(x,y)}{f_X(x)}=\frac{\frac{1}{2\pi\sqrt{1-\varrho^2}}\exp\left[-\frac{x^2-2\varrho xy+y^2}{2(1-\varrho^2)}\right]}{\frac{1}{\sqrt{2\pi}}e^{-x^2/2}}=\frac{1}{\sqrt{2\pi}\sqrt{1-\varrho^2}}\exp\left[-\frac{1}{2}\left(\frac{y-\varrho x}{\sqrt{1-\varrho^2}}\right)^2\right].$$
So $f_{Y|x}(y)$ follows a normal distribution with mean $\varrho x$ and variance $\sqrt{1-\varrho^2}$, $\mu_{Y|x}=\varrho x$. Then we can apply a inverse transformation with $\varphi_1$ and $\varphi_2$,
$$f_{X_2|x_1}(x_2)=f_{Y|x}(\varphi_2(x_2))\cdot\left|\frac{d\varphi_2(x_2)}{dx_2}\right|=\frac{1}{\sqrt{2\pi}\sigma_2\sqrt{1-\varrho^2}}\exp\left[-\frac{1}{2}\left(\frac{x_2-\mu_2-\varrho\frac{\sigma_2}{\sigma_1}(x_1-\mu_1)}{\sigma_2\sqrt{1-\varrho^2}}\right)^2\right].$$
So $f_{X_2|x_1}(x_2)$ follows a normal distribution with mean $\mu_2+\varrho\frac{\sigma_2}{\sigma_1}(x_1-\mu_1)$ and variance $\sigma_2\sqrt{1-\varrho^2}$,
$$\mu_{X_2|x_1}=\mu_2+\varrho\frac{\sigma_2}{\sigma_1}(x_1-\mu_1).$$
\item
According to the symmetric of $X_1$ and $X_2$, $f_{X_1|x_2}(x_1)$ follows a normal distribution with mean $\mu_1+\varrho\frac{\sigma_1}{\sigma_2}(x_2-\mu_2)$ and variance $\sigma_1\sqrt{1-\varrho^2}$, so
$$\mu_{X_1|x_2}=\mu_1+\varrho\frac{\sigma_1}{\sigma_2}(x_2-\mu_2)=2000+0.87\cdot\frac{\sqrt{2500}}{\sqrt{0.01}}\cdot(0.098-0.1)=1999.19{\rm~inch},$$
$$\sigma_{X_1|x_2}=\sigma_1\sqrt{1-\varrho^2}=\sqrt{2500}\cdot\sqrt{1-0.87^2}\approx24.65{\rm~inch},$$
$$P[X_1|x_2\geqslant1950]=P\left[Z\geqslant\frac{X_1|x_2-\mu_{X_1|x_2}}{\sigma_{X_1|x_2}}\right]\approx P\left[Z\geqslant\frac{1950-1999.19}{24.65}\right]\approx 1-\Phi(-1.996)\approx0.977.$$
\end{enumerate}

\subsection{Bivariate Normal Distribution as a Mixture of Independent Normal Distributions}

Let $$A=\begin{pmatrix}a_1 & b_1 \\a_2 & b_2\end{pmatrix},$$
then $$AX=\begin{pmatrix}
a_1X_1 + b_1X_2 \\
a_2X_1 + b_2X_2
\end{pmatrix}.$$

\begin{enumerate}[label=\roman*)]
\item
$$E[AX]=\begin{pmatrix}
E[a_1X_1 + b_1X_2]\\
E[a_2X_1 + b_2X_2]
\end{pmatrix}=\begin{pmatrix}
a_1E[X_1] + b_1E[X_2]\\
a_2E[X_1] + b_2E[X_2]
\end{pmatrix}=AE[X].$$
\item
Let
\begin{align*}
M&={\rm Var}[a_1X_1 + b_1X_2]=a_1^2{\rm Var}[X_1]+2a_1b_1{\rm Cov}(X_1,X_2)+b_1^2{\rm Var}[X_2],\\
N&={\rm Cov}(a_1X_1 + b_1X_2,a_2X_1 + b_2X_2)\\
&=E[(a_1X_1 + b_1X_2)(a_2X_1 + b_2X_2)]-E[a_1X_1 + b_1X_2]E[a_2X_1 + b_2X_2]\\
&=a_1a_2E[X_1^2]+(a_1b_2+a_2b_1)E[X_1X_2]+b_1b_2E[X_2^2]\\
&\quad-a_1a_2E[X_1]^2-(a_1b_2+a_2b_1)E[X_1]E[X_2]-b_1b_2E[X_2]^2\\
&=a_1a_2{\rm Var}[X_1]+(a_1b_2+a_2b_1){\rm Cov}(X_1,X_2)+b_1b_2{\rm Var}[X_2].\\
P&={\rm Var}[a_2X_1 + b_2X_2]=a_2^2{\rm Var}[X_1]+2a_2b_2{\rm Cov}(X_1,X_2)+b_2^2{\rm Var}[X_2].
\end{align*}
\begin{align*}
{\rm Var}[AX]&=\begin{pmatrix}
{\rm Var}[a_1X_1 + b_1X_2] & {\rm Cov}(a_1X_1 + b_1X_2,a_2X_1 + b_2X_2)\\
{\rm Cov}(a_2X_1 + b_2X_2,a_1X_1 + b_1X_2) & {\rm Var}[a_2X_1 + b_2X_2]
\end{pmatrix}\\
&=\begin{pmatrix}
M & N \\ N & P
\end{pmatrix}\\
&=\begin{pmatrix}a_1 & b_1 \\a_2 & b_2\end{pmatrix}
\begin{pmatrix}{\rm Var}[X_1] & {\rm Cov}(X_1,X_2) \\ {\rm Cov}(X_2,X_1) & {\rm Var}[X_2] \end{pmatrix}
\begin{pmatrix}a_1 & a_2 \\b_1 & b_2\end{pmatrix}\\
&=A{\rm Var}[X]A^T,
\end{align*}
\item

\end{enumerate}

\end{document}


